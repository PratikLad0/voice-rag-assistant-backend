services:
  # 1) Private Ollama service (internal only)
  - type: pserv                 # <-- NOT 'private'
    name: ollama
    runtime: docker
    plan: starter               # private services can't be on 'free'
    dockerfilePath: ./infra/ollama.Dockerfile
    envVars:
      - key: OLLAMA_HOST
        value: 0.0.0.0
      - key: OLLAMA_MODEL
        value: llama3:8b
    disk:
      name: ollama-cache
      mountPath: /root/.ollama  # where Ollama stores models
      sizeGB: 20

  # 2) Public FastAPI backend
  - type: web
    name: voice-rag-backend
    runtime: python
    plan: starter
    buildCommand: |
      cd backend
      pip install -r requirements.txt
      python ingest.py
    startCommand: |
      cd backend
      uvicorn app:app --host 0.0.0.0 --port $PORT
    envVars:
      - key: LLM_PROVIDER
        value: ollama
      - key: OLLAMA_BASE_URL
        value: http://ollama:11434   # internal DNS: <service name>:<port>
      - key: OLLAMA_MODEL
        value: llama3:8b
      - key: EMBEDDING_MODEL
        value: sentence-transformers/all-MiniLM-L6-v2
      - key: TOP_K
        value: "4"
      # optional: keep OpenAI vars for easy switching
      - key: OPENAI_API_KEY
        sync: false
      - key: LLM_MODEL
        value: gpt-4o-mini
